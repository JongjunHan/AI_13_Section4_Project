{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "translator.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "ncUZq8Z_CpzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fjxw1ryDQ7cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path1 = '/content/drive/MyDrive/kor-eng/kor.txt'\n",
        "\n",
        "df1 = pd.read_csv(file_path1, names = ['eng', 'kor', 'etc'], delimiter = '\\t', encoding = 'UTF-8')"
      ],
      "metadata": {
        "id": "dZlnpzxvTlj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df1.drop(columns = ['etc'])"
      ],
      "metadata": {
        "id": "d_jHd2R5TrCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path2 = '/content/drive/MyDrive/translate/conversations.csv'\n",
        "\n",
        "df2 = pd.read_csv(file_path2, encoding = 'UTF-8')"
      ],
      "metadata": {
        "id": "XKRNwurUP-P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.concat([df2['eng_sent'], df2['kor_sent']], axis = 1)\n",
        "df2.columns = ['eng', 'kor']"
      ],
      "metadata": {
        "id": "234CdgSA6MZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_kor = pd.concat([df1, df2], ignore_index = True)\n",
        "\n",
        "eng_kor"
      ],
      "metadata": {
        "id": "4M0hWAiC76gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_kor['kor'] = eng_kor['kor'].apply(lambda a : \"[start] \" + a + \" [end]\")"
      ],
      "metadata": {
        "id": "aEPWOR6rCDeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_kor.head()"
      ],
      "metadata": {
        "id": "FtbwWnyMEXMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 20\n",
        "batch_size = 64\n",
        "validation_split = 0.2\n",
        "embed_dim = 256\n",
        "latent_dim = 512\n",
        "num_heads = 4\n",
        "epochs = 15 # 에퐄을 15로 하는 이유는 30을 유지했으나 val 에서의 거의 고정된 수치와 과적합을 향하는 모습을 보여서 15로 줄임"
      ],
      "metadata": {
        "id": "zhauYyNPFqTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "print(strip_chars)\n",
        "\n",
        "def eng_standardize(input_text):\n",
        "\n",
        "    output_text = tf.strings.lower(input_text)\n",
        "\n",
        "    return tf.strings.regex_replace(output_text, '[%s]' % re.escape(strip_chars), '')\n",
        "\n",
        "def kor_standardize(input_text):\n",
        "\n",
        "    return tf.strings.regex_replace(input_text, '[%s]' %re.escape(strip_chars), '')\n",
        "\n",
        "eng_vector = TextVectorization(\n",
        "    max_tokens = vocab_size, \n",
        "    output_mode = 'int', \n",
        "    output_sequence_length = sequence_length,\n",
        "    standardize = eng_standardize)\n",
        "\n",
        "kor_vector = TextVectorization(\n",
        "    max_tokens = vocab_size,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = sequence_length + 1,\n",
        "    standardize = kor_standardize)\n",
        "\n",
        "eng_vector.adapt(list(eng_kor['eng']))\n",
        "kor_vector.adapt(list(eng_kor['kor']))"
      ],
      "metadata": {
        "id": "5WdNLPPkEaOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(english, korean):\n",
        "\n",
        "    eng = eng_vector(english)\n",
        "    kor = kor_vector(korean)\n",
        "\n",
        "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": kor[:, :-1]}, kor[:, 1:])\n",
        "\n",
        "def make_df(df, batch_size, mode):\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(df['eng']), list(df['kor'])))\n",
        "    \n",
        "    if mode == \"train\":\n",
        "       dataset = dataset.shuffle(256)\n",
        "       \n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(preprocess)\n",
        "    dataset = dataset.prefetch(16).cache()\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "train, test = train_test_split(eng_kor, test_size = validation_split)\n",
        "train.shape, test.shape"
      ],
      "metadata": {
        "id": "JiayRX0GGUcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = make_df(train, batch_size = batch_size, mode = 'train')\n",
        "X_test = make_df(test, batch_size = batch_size, mode = 'test')\n",
        "\n",
        "for batch in X_train.take(1):\n",
        "    print(batch)"
      ],
      "metadata": {
        "id": "F3uOiszNG22-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(TransformerDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)"
      ],
      "metadata": {
        "id": "Mf63vhrwG73H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs = keras.Input(shape = (None,), dtype = 'int64', name = 'encoder_inputs')\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape = (None,), dtype = 'int64', name = 'decoder_inputs')\n",
        "encoded_seq_inputs = keras.Input(shape = (None, embed_dim), name = 'decoder_state_inputs')\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation = 'softmax')(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "model = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name = 'transformer')"
      ],
      "metadata": {
        "id": "aKMw7R7dHLtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "r76TKY1UHV6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "dFvSHU0KHdJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "AqqHgcFUHfxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, epochs = epochs, validation_data = X_test)"
      ],
      "metadata": {
        "id": "YhtUdvylHiHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kor_vocab = kor_vector.get_vocabulary()\n",
        "kor_index_lookup = dict(zip(range(len(kor_vocab)), kor_vocab))\n",
        "\n",
        "def decode_sequence(model, input_sentence):\n",
        "    \n",
        "    tokenized_input_sentence = eng_vector([input_sentence])\n",
        "    decoded_sentence = '[start]'\n",
        "    \n",
        "    for i in range(sequence_length):\n",
        "        tokenized_target_sentence = kor_vector([decoded_sentence])[:, :-1]\n",
        "        predictions = model([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = kor_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "            \n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "ZrfmqspOKbOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in np.random.choice(len(eng_kor), 10):\n",
        "    a = eng_kor.iloc[i]\n",
        "    translated = decode_sequence(model, a['eng'])\n",
        "    \n",
        "    print(\"English:\", a['eng'])\n",
        "    print(\"korean:\", a['kor'])\n",
        "    print(\"Translated:\", translated)"
      ],
      "metadata": {
        "id": "yvU7o_yCKqx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decode_sequence(model, 'cat is cute')"
      ],
      "metadata": {
        "id": "uKTbc5UpK84B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "적은 데이터 양과 부족한 실력에 의해 좋은 번역 결과물을 얻을 수 없었다.\n",
        "추후 말뭉치의 수를 늘려가고 자체 학습을 하여 방법을 변화 시켜가며 모델을 업데이트 혹은 새로운 모델을 만들어서 해봐야 겠다."
      ],
      "metadata": {
        "id": "P9rXeJxw5zSc"
      }
    }
  ]
}